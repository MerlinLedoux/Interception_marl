{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook utilisation de torch pour du MARL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxime version du note book je vais jouer sur les hyperparametre l'utilisation d'autre modele, d'autre environement pour voir tous ce qu'il est possible de faire et comprendre comment l'adapter a mon cas.\n",
    "Import de la bibliotheque torchrl et de l'ensemble des pakage nécessaire a un systeme multi agent qui tourne sur GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooEnv\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection des hyperparametre pour l'entrainement on commence par definie avec quoi sera entrainer le modele (cpu ou gpu) en fonction de ce qui est disponible sue la machine si le gpu est diponible on va préferer l'utilise car ca permet de grandement accelerer l'apprentisage. Ensuite on a l'echantilonage le nombre d'action par bath le nombre d'iteration. Les hyperparamétre general de l'entrainement et ce spécifique a la methode PPO utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(device)\n",
    "\n",
    "vmas_device = device\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 50  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "# disable log-prob aggregation\n",
    "set_composite_lp_aggregate(False).set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix et parametrage de l'environement utiliser. Ici il s'agit de l'environement de navigation de Vmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000  # Episode steps before done\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
    "scenario_name = \"balance\"\n",
    "n_agents = 2\n",
    "\n",
    "env =PettingZooE\n",
    "\n",
    "\n",
    "env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,  # VMAS supports both continuous and discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    # Scenario kwargs\n",
    "    n_agents=n_agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    agents: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([6, 2, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([6, 2, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([6, 2, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([6, 2]),\n",
      "        data_cls=None),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([6]),\n",
      "    data_cls=None)\n",
      "reward_spec: Composite(\n",
      "    agents: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([6, 2, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([6, 2]),\n",
      "        data_cls=None),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([6]),\n",
      "    data_cls=None)\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([6, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([6, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([6]),\n",
      "    data_cls=None)\n",
      "observation_spec: Composite(\n",
      "    agents: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([6, 2, 16]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([6, 2, 16]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([6, 2, 16]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        info: Composite(\n",
      "            pos_rew: UnboundedContinuous(\n",
      "                shape=torch.Size([6, 2, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            ground_rew: UnboundedContinuous(\n",
      "                shape=torch.Size([6, 2, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            device=cpu,\n",
      "            shape=torch.Size([6, 2]),\n",
      "            data_cls=None),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([6, 2]),\n",
      "        data_cls=None),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([6]),\n",
      "    data_cls=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", env.full_action_spec)\n",
    "print(\"reward_spec:\", env.full_reward_spec)\n",
    "print(\"done_spec:\", env.full_done_spec)\n",
    "print(\"observation_spec:\", env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('agents', 'action')]\n",
      "reward_keys: [('agents', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", env.action_keys)\n",
    "print(\"reward_keys:\", env.reward_keys)\n",
    "print(\"done_keys:\", env.done_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout d'une nouvelle sortie a l'environement. On somme les reward pour chaque agent au fils des iteration ca permet d'avoir un suivie des performence de l'agent a chaque étape de l'entrainement et donc pouvoir suivre si l'entrainement ce passe bien ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2025-07-31 13:50:00,873 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pour voir la forme de la sortie dans une trajectoire complete (rollout). Ici la trajectoire fait 5 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([6, 5, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        ground_rew: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        pos_rew: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([6, 5, 2]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([6, 5, 2, 16]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([6, 5, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([6, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agents: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        info: TensorDict(\n",
      "                            fields={\n",
      "                                ground_rew: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                pos_rew: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                            batch_size=torch.Size([6, 5, 2]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([6, 5, 2, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([6, 5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([6, 5, 2]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([6, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([6, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([6, 5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([6, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([6, 5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([6, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrage du reseux utiliser pour l'apprentisage. On utilise le modele multi agent de torchrl et on configure les parametre d'apprentisage : nombre d'agent, décentraliser, une polituqye partager. Et la forme des sortie pour qu'elles s'adapte a notre modéle ici on utilise PPO donc pour chaque actionn il faut deux sorties une pour la moyenne et une pour l'écart type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = True\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=2\n",
    "        * env.full_action_spec[env.action_key].shape[-1],  # 2 * n_actions_per_agents\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec_unbatched,\n",
    "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.full_action_spec_unbatched[env.action_key].space.low,\n",
    "        \"high\": env.full_action_spec_unbatched[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    ")  # we'll need the log-prob for the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,  # 1 value per agent\n",
    "    n_agents=env.n_agents,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([6, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                action_log_prob: Tensor(shape=torch.Size([6, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        ground_rew: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        pos_rew: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([6, 2]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                loc: Tensor(shape=torch.Size([6, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([6, 2, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                scale: Tensor(shape=torch.Size([6, 2, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([6, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([6, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([6, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([6]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                episode_reward: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        ground_rew: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        pos_rew: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([6, 2]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([6, 2, 16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                state_value: Tensor(shape=torch.Size([6, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([6, 2]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([6, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([6, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([6]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy(env.reset()))\n",
    "print(\"Running value:\", critic(env.reset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SyncDataCollector est un object de data collector qui est la pour faire fonctioner l'environement avec la politique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecte les résultats de chaque pas pour les stocker. Dans ce cas la ce n'est pas nécessaire car on utilise PPO une methode on policy, donc en theorie il n'est pas nécessaire de mémoriser les données. Mais pour que le systeme soit adaptable et qu'il soit facile de changer la politique utiliser on le met. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    value=(\"agents\", \"state_value\"),\n",
    "    # These last 2 keys will be expanded to match the reward shape\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 0:   0%|          | 0/10 [00:09<?, ?it/s]\n",
      "episode_reward_mean = 44.31297302246094:  90%|█████████ | 9/10 [02:18<00:15, 15.39s/it] "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m      8\u001b[0m         frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrender_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_cast_to_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\common.py:3383\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[1;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[0m\n\u001b[0;32m   3377\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_stop_early(\n\u001b[0;32m   3378\u001b[0m         break_when_all_done\u001b[38;5;241m=\u001b[39mbreak_when_all_done,\n\u001b[0;32m   3379\u001b[0m         break_when_any_done\u001b[38;5;241m=\u001b[39mbreak_when_any_done,\n\u001b[0;32m   3380\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3381\u001b[0m     )\n\u001b[0;32m   3382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3383\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3384\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;28;01mif\u001b[39;00m tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m   3385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contiguous:\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\common.py:3607\u001b[0m, in \u001b[0;36mEnvBase._rollout_nonstop\u001b[1;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[0;32m   3605\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   3606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3607\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensordicts\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36mrender_callback\u001b[1;34m(env, _)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m frame_count\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m<\u001b[39m max_rendered_frames:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\vmas\\simulator\\environment\\environment.py:792\u001b[0m, in \u001b[0;36mEnvironment.render\u001b[1;34m(self, mode, env_index, agent_index_focus, visualize_when_rgb, plot_position_function, plot_position_function_precision, plot_position_function_range, plot_position_function_cmap_range, plot_position_function_cmap_alpha, plot_position_function_cmap_name)\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer\u001b[38;5;241m.\u001b[39madd_onetime_list(entity\u001b[38;5;241m.\u001b[39mrender(env_index\u001b[38;5;241m=\u001b[39menv_index))\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# render to display or array\u001b[39;00m\n\u001b[1;32m--> 792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\vmas\\simulator\\rendering.py:172\u001b[0m, in \u001b[0;36mViewer.render\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_rgb_array:\n\u001b[0;32m    171\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array()\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monetime_geoms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:384\u001b[0m, in \u001b[0;36mWin32Window.flip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverlay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransparent\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_transparency()\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flip'"
     ]
    }
   ],
   "source": [
    "frame_count = 0\n",
    "max_rendered_frames = 1000\n",
    "\n",
    "def render_callback(env, _):\n",
    "    global frame_count\n",
    "    if frame_count < max_rendered_frames:\n",
    "        env.render()\n",
    "        frame_count += 1\n",
    "\n",
    "env.rollout(\n",
    "    max_steps=max_steps,\n",
    "    policy=policy,\n",
    "    callback=render_callback,\n",
    "    auto_cast_to_device=True,\n",
    "    break_when_any_done=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythorch_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
