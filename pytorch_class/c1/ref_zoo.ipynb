{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook entrainement environement pettingzoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook je presente l'entrainement de l'environement simple reference v3 de la bibliotheque mpe de petting zoo. C'est un environement coopératif dans lequel on a 3 mark (rouge, bleu, vert) et 2 agents d'une des trois couleurs les agents on pour but de ce diriger vers la marque de leurs couleurs (les deux agents peuvent etre de la meme couleurs). Les agents connaise la position des marks de l'autre agent mais pas la leur. Ils parle et ecoute tous les deux. Leurs but et de maximiser une récompence qui est composé de leurs distance a leurs objectif et de la distance moyen, ca les encourage a apprendre à bien communiquer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps on importe l'ensemble des dependance nécessaire au bon fonctionement de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:36: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. This is likely due to a discrepancy between your package version and the PyTorch version. Make sure both are compatible. Usually, torchrl majors follow the pytorch majors within a few days around the release. For instance, TorchRL 0.5 requires PyTorch 2.4.0, and TorchRL 0.6 requires PyTorch 2.5.0.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "# from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooEnv\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamétres\n",
    "\n",
    "On définie les resource matériel qui seront utiliser pour l'entrainement et la simulation (environement). On définie aussi les hyperparamétres pour l'échantilonage et l'entrainement. L'agorithme que l'on utilisera sera PPO un algorithme on policy qui supporte les espace d'entrée et de sortie continue et qui offre un entrainement trés stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(device)\n",
    "\n",
    "vmas_device = device\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000    # Number of team frames collected per training iteration\n",
    "n_iters = 100               # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30             # Number of optimization steps per training iteration\n",
    "minibatch_size = 400        # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4                   # Learning rate\n",
    "max_grad_norm = 1.0         # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2          # clip value for PPO loss\n",
    "gamma = 0.99                # discount factor\n",
    "lmbda = 0.9                 # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4          # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "# disable log-prob aggregation\n",
    "set_composite_lp_aggregate(False).set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envionement \n",
    "L'environement utiliser est importer depuis pettingzoo il s'agit du \"simple_reference_v3\" on le paramétre de maniére a ce qu'il fonctione en parallele avec un ration de entre les récompence global et local, un maximum de 25 cycle par simulation et des actions continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:1019: UserWarning: PettingZoo failed to load all modules with error message No module named 'multi_agent_ale_py', trying to load individual modules.\n",
      "  warnings.warn(\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:52: UserWarning: SISL environments failed to load with error message No module named 'Box2D'.\n",
      "  warnings.warn(f\"SISL environments failed to load with error message {err}.\")\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:58: UserWarning: Classic environments failed to load with error message No module named 'chess'.\n",
      "  warnings.warn(f\"Classic environments failed to load with error message {err}.\")\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:64: UserWarning: Atari environments failed to load with error message No module named 'multi_agent_ale_py'.\n",
      "  warnings.warn(f\"Atari environments failed to load with error message {err}.\")\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:70: UserWarning: Butterfly environments failed to load with error message No module named 'pymunk'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = PettingZooEnv(\n",
    "    task=\"simple_reference_v3\",\n",
    "    parallel=True,                  # Les actions des deux agents sont pris simultanement\n",
    "    seed=seed,                      \n",
    "    local_ratio=0.5,                # Ratio entre la récompence global et la récompence local\n",
    "    max_cycles=25,                  # Nombre d'action avant que l'environemen sois terminer\n",
    "    continuous_actions=False,        # L'espace des action est continue\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    agent: Composite(\n",
      "        action: Categorical(\n",
      "            shape=torch.Size([2]),\n",
      "            space=CategoricalBox(n=50),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=None,\n",
      "        shape=torch.Size([2]),\n",
      "        data_cls=None),\n",
      "    device=None,\n",
      "    shape=torch.Size([]),\n",
      "    data_cls=None)\n",
      "action_keys: [('agent', 'action')]\n"
     ]
    }
   ],
   "source": [
    "# print(f\"group_map: {env.group_map}\")\n",
    "print(\"action_spec:\", env.full_action_spec)\n",
    "# print(\"reward_spec:\", env.full_reward_spec)\n",
    "# print(\"done_spec:\", env.full_done_spec)\n",
    "# print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"action_keys:\", env.action_keys)\n",
    "# print(\"reward_keys:\", env.reward_keys)\n",
    "# print(\"done_keys:\", env.done_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute une sortie a l'environement qui somme l'ensemble des récompence de la sumulation autrement on ne peut avoir que la récompence de l'action et on ne dispose d'aucun suivie global des recompence. Cette récompence est tres importante pour comparer les performances des different modéle car elle permet d'avoir une vrai idée sur les capacité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agent\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2025-08-01 15:21:24,911 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simumation de test pour voir si tous ce passe comme prévu dans l'environement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([5, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 2, 21]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5, 2]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agent: TensorDict(\n",
      "                    fields={\n",
      "                        done: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        episode_reward: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([5, 2, 21]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([5, 2, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([5, 2]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politique \n",
    "\n",
    "L'agorithme utilisé pour l'entrainement est PPO. Cette algorithme implique par sont fonctionement qu'il y a plsieur mobule nécessaire en plus du réseaux il est necessaire de séparer les moyenne et les ecart type (policy module) les compiner pour obtenir les actions (policy) et on a besoin d'un raiseau critique pour le calcul de la loss qui est nécessaire au bonee entrainment du systeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "share_parameters_policy = True\n",
    "\n",
    "# reseaux pour des actions continue\n",
    "# policy_net = torch.nn.Sequential(\n",
    "#     MultiAgentMLP(\n",
    "#         n_agent_inputs=env.observation_spec[\"agent\", \"observation\"].shape[-1],  # n_obs_per_agent\n",
    "#         n_agent_outputs=2 * env.full_action_spec[env.action_key].shape[-1],  # 2 * n_actions_per_agents\n",
    "#         n_agents=2,\n",
    "#         centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "#         share_params=share_parameters_policy,\n",
    "#         device=device,\n",
    "#         depth=2,\n",
    "#         num_cells=256,\n",
    "#         activation_class=torch.nn.Tanh,\n",
    "#     ),\n",
    "#     NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    "# )\n",
    "\n",
    "# reseaux pour des actions discrete\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agent\", \"observation\"].shape[-1],  # n_obs_per_agent\n",
    "        n_agent_outputs=env.full_action_spec[env.action_key].space.n, # 50\n",
    "        n_agents=2,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue \n",
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agent\", \"observation\")],\n",
    "    out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    ")\n",
    "\n",
    "# Discrete\n",
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agent\", \"observation\")],\n",
    "    out_keys=[(\"agent\", \"logits\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec_unbatched,\n",
    "    in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.full_action_spec_unbatched[env.action_key].space.low,\n",
    "        \"high\": env.full_action_spec_unbatched[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    ")  # we'll need the log-prob for the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agent\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,  # 1 value per agent\n",
    "    n_agents=2,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agent\", \"observation\")],\n",
    "    out_keys=[(\"agent\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Running policy:\", policy(env.reset()))\n",
    "# print(\"Running value:\", critic(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_per_batch : 6000, total_frames : 600000\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n",
    "\n",
    "print(f\"frames_per_batch : {frames_per_batch}, total_frames : {total_frames}\")\n",
    "print(total_frames/frames_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\objectives\\ppo.py:450: DeprecationWarning: 'entropy_coef' is deprecated and will be removed in torchrl v0.11. Please use 'entropy_coeff' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    value=(\"agent\", \"state_value\"),\n",
    "    # These last 2 keys will be expanded to match the reward shape\n",
    "    done=(\"agent\", \"done\"),\n",
    "    terminated=(\"agent\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 0:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "action is not in action space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mn_iters, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_mean = 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m episode_reward_mean_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tensordict_data \u001b[38;5;129;01min\u001b[39;00m collector:\n\u001b[0;32m      5\u001b[0m     tensordict_data\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m      6\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m         tensordict_data\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mexpand(tensordict_data\u001b[38;5;241m.\u001b[39mget_item_shape((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39mreward_key))),\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     tensordict_data\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m     12\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminated\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m         tensordict_data\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminated\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mexpand(tensordict_data\u001b[38;5;241m.\u001b[39mget_item_shape((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39mreward_key))),\n\u001b[0;32m     16\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\collectors\\collectors.py:341\u001b[0m, in \u001b[0;36mDataCollectorBase.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[TensorDictBase]:\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 341\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator()\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\collectors\\collectors.py:1256\u001b[0m, in \u001b[0;36mSyncDataCollector.iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m   1255\u001b[0m     torchrl_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollector: rollout.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1256\u001b[0m tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordict_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;66;03m# if a replay buffer is passed and self.extend_buffer=False, there is no tensordict_out\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;66;03m#  frames are updated within the rollout function\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m     torchrl_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollector: No tensordict_out. Yielding.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\_utils.py:661\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[1;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\collectors\\collectors.py:1521\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[1;32m-> 1521\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\common.py:3655\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   3654\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 3655\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m tensordict_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_mdp(tensordict)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\common.py:2101\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   2098\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2101\u001b[0m     next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2102\u001b[0m     next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2104\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m   2106\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\transforms\\transforms.py:1166\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         tensordict_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m     next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m         next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   1172\u001b[0m             next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m   1173\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:616\u001b[0m, in \u001b[0;36mPettingZooWrapper._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_step\u001b[39m(\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    607\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    608\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorDictBase:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel:\n\u001b[0;32m    610\u001b[0m         (\n\u001b[0;32m    611\u001b[0m             observation_dict,\n\u001b[0;32m    612\u001b[0m             rewards_dict,\n\u001b[0;32m    613\u001b[0m             terminations_dict,\n\u001b[0;32m    614\u001b[0m             truncations_dict,\n\u001b[0;32m    615\u001b[0m             info_dict,\n\u001b[1;32m--> 616\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m         (\n\u001b[0;32m    619\u001b[0m             observation_dict,\n\u001b[0;32m    620\u001b[0m             rewards_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m             info_dict,\n\u001b[0;32m    624\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_aec(tensordict)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:763\u001b[0m, in \u001b[0;36mPettingZooWrapper._step_parallel\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m    760\u001b[0m         action \u001b[38;5;241m=\u001b[39m _extract_nested_with_index(group_action_np, index)\n\u001b[0;32m    761\u001b[0m         action_dict[agent] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m--> 763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\pettingzoo\\utils\\conversions.py:207\u001b[0m, in \u001b[0;36maec_to_parallel_wrapper.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m         )\n\u001b[0;32m    206\u001b[0m obs, rew, termination, truncation, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mlast()\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m    209\u001b[0m     rewards[agent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mrewards[agent]\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:96\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:47\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\assert_out_of_bounds.py:18\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActionType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m---> 18\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection]\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection]\n\u001b[0;32m     22\u001b[0m         )\n\u001b[0;32m     23\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection)\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m     24\u001b[0m         action\n\u001b[0;32m     25\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction is not in action space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mAssertionError\u001b[0m: action is not in action space"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agent\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agent\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agent\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agent\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-17.1088383702198\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "from pettingzoo.mpe import simple_reference_v3\n",
    "\n",
    "env = simple_reference_v3.env(render_mode=\"human\", continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "obs_vide = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "obs_agent1 = torch.tensor(obs_vide, dtype=torch.float32) \n",
    "sum_reward = 0\n",
    "\n",
    "\n",
    "# Pour avoir acces aux fonction render de petting zoo il faut etre en mode ACS donc on doit traiter les agent 1 par 1\n",
    "# C'est ce que permet cette boucle on traite l'agent 0 puis 1 en boucle\n",
    "for agent in env.agent_iter():\n",
    "    env.render()\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    sum_reward += reward/2\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        if agent == \"agent_0\":\n",
    "            obs_agent0 = torch.tensor(observation, dtype=torch.float32)\n",
    "        else:\n",
    "            obs_agent1 = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        obs_agents = torch.stack([obs_agent0, obs_agent1], dim=0)\n",
    "        input_td = TensorDict(\n",
    "            source={(\"agent\", \"observation\"): obs_agents},\n",
    "            batch_size=[2], \n",
    "        ) \n",
    "        out = policy(input_td)\n",
    "\n",
    "        if agent == \"agent_0\":\n",
    "            action = out.get((\"agent\", \"action\"))[0]\n",
    "        else:\n",
    "            action = out.get((\"agent\", \"action\"))[1]\n",
    "        action = action.detach().cpu().numpy()\n",
    "    env.step(action)\n",
    "env.close()\n",
    "\n",
    "print(sum_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythorch_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
