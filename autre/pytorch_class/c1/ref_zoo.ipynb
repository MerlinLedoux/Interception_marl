{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook entrainement environement pettingzoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook je presente l'entrainement de l'environement simple reference v3 de la bibliotheque mpe de petting zoo. C'est un environement coopératif dans lequel on a 3 mark (rouge, bleu, vert) et 2 agents d'une des trois couleurs les agents on pour but de ce diriger vers la marque de leurs couleurs (les deux agents peuvent etre de la meme couleurs). Les agents connaise la position des marks de l'autre agent mais pas la leur. Ils parle et ecoute tous les deux. Leurs but et de maximiser une récompence qui est composé de leurs distance a leurs objectif et de la distance moyen, ca les encourage a apprendre à bien communiquer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps on importe l'ensemble des dependance nécessaire au bon fonctionement de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "# from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "# from torchrl.envs.libs.pettingzoo import PettingZooEnv\n",
    "from torchrl.envs import PettingZooEnv\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamétres\n",
    "\n",
    "On définie les resource matériel qui seront utiliser pour l'entrainement et la simulation (environement). On définie aussi les hyperparamétres pour l'échantilonage et l'entrainement. L'agorithme que l'on utilisera sera PPO un algorithme on policy qui supporte les espace d'entrée et de sortie continue et qui offre un entrainement trés stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(device)\n",
    "\n",
    "vmas_device = device\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000    # Number of team frames collected per training iteration\n",
    "n_iters = 150               # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30             # Number of optimization steps per training iteration\n",
    "minibatch_size = 400        # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4                   # Learning rate\n",
    "max_grad_norm = 1.0         # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2          # clip value for PPO loss\n",
    "gamma = 0.99                # discount factor\n",
    "lmbda = 0.9                 # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4          # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "# disable log-prob aggregation\n",
    "set_composite_lp_aggregate(False).set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envionement \n",
    "L'environement utiliser est importer depuis pettingzoo il s'agit du \"simple_reference_v3\" on le paramétre de maniére a ce qu'il fonctione en parallele avec un ration de entre les récompence global et local, un maximum de 25 cycle par simulation et des actions continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:1019: UserWarning: PettingZoo failed to load all modules with error message No module named 'multi_agent_ale_py', trying to load individual modules.\n",
      "  warnings.warn(\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:52: UserWarning: SISL environments failed to load with error message No module named 'Box2D'.\n",
      "  warnings.warn(f\"SISL environments failed to load with error message {err}.\")\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:58: UserWarning: Classic environments failed to load with error message No module named 'chess'.\n",
      "  warnings.warn(f\"Classic environments failed to load with error message {err}.\")\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:64: UserWarning: Atari environments failed to load with error message No module named 'multi_agent_ale_py'.\n",
      "  warnings.warn(f\"Atari environments failed to load with error message {err}.\")\n",
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:70: UserWarning: Butterfly environments failed to load with error message No module named 'pymunk'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = PettingZooEnv(\n",
    "    # task=\"simple_reference_v3\",\n",
    "    task=\"simple_spread_v3\",\n",
    "    parallel=True,                  # Les actions des deux agents sont pris simultanement\n",
    "    seed=seed,                      \n",
    "    local_ratio=0.5,                # Ratio entre la récompence global et la récompence local\n",
    "    max_cycles=25,                  # Nombre d'action avant que l'environemen sois terminer\n",
    "    continuous_actions=True,        # L'espace des action est continue\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PettingZooEnv()\n",
      "<class 'torchrl.envs.libs.pettingzoo.PettingZooEnv'>\n"
     ]
    }
   ],
   "source": [
    "print(env)\n",
    "print(type(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_spec: Composite(\n",
      "    agent: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([3, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=None,\n",
      "        shape=torch.Size([3]),\n",
      "        data_cls=None),\n",
      "    device=None,\n",
      "    shape=torch.Size([]),\n",
      "    data_cls=None)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"group_map: {env.group_map}\")\n",
    "# print(\"action_spec:\", env.full_action_spec)\n",
    "print(\"reward_spec:\", env.full_reward_spec)\n",
    "# print(\"done_spec:\", env.full_done_spec)\n",
    "# print(\"observation_spec:\", env.observation_spec)\n",
    "# print(\"action_keys:\", env.action_keys)\n",
    "# print(\"reward_keys:\", env.reward_keys)\n",
    "# print(\"done_keys:\", env.done_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute une sortie a l'environement qui somme l'ensemble des récompence de la sumulation autrement on ne peut avoir que la récompence de l'action et on ne dispose d'aucun suivie global des recompence. Cette récompence est tres importante pour comparer les performances des different modéle car elle permet d'avoir une vrai idée sur les capacité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agent\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m2025-08-04 11:50:45,299 [torchrl][INFO]\u001b[0m    check_env_specs succeeded!\u001b[92m [END]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simumation de test pour voir si tous ce passe comme prévu dans l'environement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agent: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([5, 3, 18]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5, 3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agent: TensorDict(\n",
      "                    fields={\n",
      "                        done: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        episode_reward: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([5, 3, 18]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([5, 3]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([5]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politique \n",
    "\n",
    "L'agorithme utilisé pour l'entrainement est PPO. Cette algorithme implique par sont fonctionement qu'il y a plsieur mobule nécessaire en plus du réseaux il est necessaire de séparer les moyenne et les ecart type (policy module) les compiner pour obtenir les actions (policy) et on a besoin d'un raiseau critique pour le calcul de la loss qui est nécessaire au bonee entrainment du systeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "share_parameters_policy = True\n",
    "\n",
    "# reseaux pour des actions continue\n",
    "# policy_net = torch.nn.Sequential(\n",
    "#     MultiAgentMLP(\n",
    "#         n_agent_inputs=env.observation_spec[\"agent\", \"observation\"].shape[-1],  # n_obs_per_agent\n",
    "#         n_agent_outputs=2 * env.full_action_spec[env.action_key].shape[-1],  # 2 * n_actions_per_agents\n",
    "#         n_agents=2,\n",
    "#         centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "#         share_params=share_parameters_policy,\n",
    "#         device=device,\n",
    "#         depth=2,\n",
    "#         num_cells=256,\n",
    "#         activation_class=torch.nn.Tanh,\n",
    "#     ),\n",
    "#     NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    "# )\n",
    "\n",
    "# reseaux pour des actions discrete\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agent\", \"observation\"].shape[-1],  # n_obs_per_agent\n",
    "        n_agent_outputs=env.full_action_spec[env.action_key].space.n, # 50\n",
    "        n_agents=3,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(env.observation_spec[\"agent\", \"observation\"].shape[-1])\n",
    "print(env.full_action_spec[env.action_key].space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue \n",
    "# policy_module = TensorDictModule(\n",
    "#     policy_net,\n",
    "#     in_keys=[(\"agent\", \"observation\")],\n",
    "#     out_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "# )\n",
    "\n",
    "# Discrete\n",
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agent\", \"observation\")],\n",
    "    out_keys=[(\"agent\", \"logits\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue\n",
    "# policy = ProbabilisticActor(\n",
    "#     module=policy_module,\n",
    "#     spec=env.action_spec_unbatched,\n",
    "#     in_keys=[(\"agent\", \"loc\"), (\"agent\", \"scale\")],\n",
    "#     out_keys=[env.action_key],\n",
    "#     distribution_class=TanhNormal,\n",
    "#     distribution_kwargs={\n",
    "#         \"low\": env.full_action_spec_unbatched[env.action_key].space.low,\n",
    "#         \"high\": env.full_action_spec_unbatched[env.action_key].space.high,\n",
    "#     },\n",
    "#     return_log_prob=True,\n",
    "# )  # we'll need the log-prob for the PPO loss\n",
    "\n",
    "# Discrete\n",
    "\n",
    "from torchrl.modules.tensordict_module.actors import Categorical\n",
    "\n",
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec_unbatched,\n",
    "    in_keys=[(\"agent\", \"logits\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agent\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,  # 1 value per agent\n",
    "    n_agents=3,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agent\", \"observation\")],\n",
    "    out_keys=[(\"agent\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Running policy:\", policy(env.reset()))\n",
    "# print(\"Running value:\", critic(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames_per_batch : 6000, total_frames : 900000\n",
      "150.0\n"
     ]
    }
   ],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n",
    "\n",
    "print(f\"frames_per_batch : {frames_per_batch}, total_frames : {total_frames}\")\n",
    "print(total_frames/frames_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\objectives\\ppo.py:450: DeprecationWarning: 'entropy_coef' is deprecated and will be removed in torchrl v0.11. Please use 'entropy_coeff' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    value=(\"agent\", \"state_value\"),\n",
    "    # These last 2 keys will be expanded to match the reward shape\n",
    "    done=(\"agent\", \"done\"),\n",
    "    terminated=(\"agent\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = -16.114309310913086: 100%|██████████| 150/150 [1:05:21<00:00, 25.28s/it]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agent\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agent\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agent\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agent\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceullule de visualisation pour l'environement simple_spread_v3. Si il est run avec un autre environement il fera s'arréter le python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FX643778\\AppData\\Local\\Temp\\ipykernel_5216\\1244714526.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.nn.functional.one_hot(torch.tensor(action), num_classes=5).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "-19.41493664296949\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "\n",
    "env = simple_spread_v3.env(render_mode=\"human\", continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "obs_vide = torch.zeros(18, dtype=torch.float32)\n",
    "obs_agent0 = obs_vide.clone()\n",
    "obs_agent1 = obs_vide.clone()\n",
    "obs_agent2 = obs_vide.clone()\n",
    "\n",
    "sum_reward = 0\n",
    "step = 0\n",
    "\n",
    "# Pour avoir acces aux fonction render de petting zoo il faut etre en mode ACS donc on doit traiter les agent 1 par 1\n",
    "# C'est ce que permet cette boucle on traite l'agent 0 puis 1 en boucle\n",
    "for agent in env.agent_iter():\n",
    "    print(step)\n",
    "    step += 1\n",
    "    env.render()\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    sum_reward += reward / 3\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        if agent == \"agent_0\":\n",
    "            obs_agent0 = torch.tensor(observation, dtype=torch.float32)\n",
    "        if agent == \"agent_1\":\n",
    "            obs_agent1 = torch.tensor(observation, dtype=torch.float32)\n",
    "        else:\n",
    "            obs_agent2 = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        obs_agents = torch.stack([obs_agent0, obs_agent1, obs_agent2], dim=0)\n",
    "        input_td = TensorDict(\n",
    "            source={(\"agent\", \"observation\"): obs_agents},\n",
    "            batch_size=[3], \n",
    "        ) \n",
    "        out = policy(input_td)\n",
    "\n",
    "        if agent == \"agent_0\":\n",
    "            action = out.get((\"agent\", \"action\"))[0]\n",
    "        elif agent == \"agent_1\":\n",
    "            action = out.get((\"agent\", \"action\"))[1]\n",
    "        else:\n",
    "            action = out.get((\"agent\", \"action\"))[2]\n",
    "\n",
    "        action = torch.nn.functional.one_hot(torch.tensor(action), num_classes=5).float()\n",
    "    env.step(action)\n",
    "env.close()\n",
    "\n",
    "print(sum_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 1.0, (5,), float32)\n"
     ]
    }
   ],
   "source": [
    "from tensordict import TensorDict\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "\n",
    "env = simple_spread_v3.env(render_mode=\"human\", continuous_actions=True)\n",
    "env.reset()\n",
    "\n",
    "print(env.action_space(\"agent_0\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceullule de visualisation pour l'environement simple_reference_v3. Si il est run avec un autre environement il fera s'arréter le python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multi-agent network expected input with shape[-2]=3, but got torch.Size([2, 21])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: TensorDictModule failed with operation\n    Sequential(\n      (0): MultiAgentMLP(\n          MLP(\n            (0): Linear(in_features=18, out_features=256, bias=True)\n            (1): Tanh()\n            (2): Linear(in_features=256, out_features=256, bias=True)\n            (3): Tanh()\n            (4): Linear(in_features=256, out_features=5, bias=True)\n          ),\n          n_agents=3,\n          share_params=True,\n          centralized=False,\n          agent_dim=-2)\n    )\n    in_keys=[('agent', 'observation')]\n    out_keys=[('agent', 'logits')].",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m\n\u001b[0;32m     28\u001b[0m obs_agents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([obs_agent0, obs_agent1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m input_td \u001b[38;5;241m=\u001b[39m TensorDict(\n\u001b[0;32m     30\u001b[0m     source\u001b[38;5;241m=\u001b[39m{(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m): obs_agents},\n\u001b[0;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m], \n\u001b[0;32m     32\u001b[0m ) \n\u001b[1;32m---> 33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_td\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     36\u001b[0m     action \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:328\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\utils.py:372\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m _skip_existing\u001b[38;5;241m.\u001b[39mget_mode()\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 372\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m     _skip_existing\u001b[38;5;241m.\u001b[39mset_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:1348\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1345\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed while executing module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_num_or_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Scroll up for more info.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1346\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m     tensordict_exec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dist_params(tensordict_exec, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1349\u001b[0m     tensordict_exec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_module(\n\u001b[0;32m   1350\u001b[0m         tensordict_exec, _requires_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_sample\n\u001b[0;32m   1351\u001b[0m     )\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\probabilistic.py:1100\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.get_dist_params\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a default interaction in the modules.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_interaction_type(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m-> 1100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tds(tensordict, tensordict_out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:328\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\utils.py:372\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m _skip_existing\u001b[38;5;241m.\u001b[39mget_mode()\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 372\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m     _skip_existing\u001b[38;5;241m.\u001b[39mset_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\sequence.py:627\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[1;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_iter():\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 627\u001b[0m         tensordict_exec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_module(\n\u001b[0;32m    628\u001b[0m             module, tensordict_exec, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    629\u001b[0m         )\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _has_py311_or_greater:\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\sequence.py:573\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[1;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_module\u001b[39m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    566\u001b[0m     module: TensorDictModuleBase,\n\u001b[0;32m    567\u001b[0m     tensordict: TensorDictBase,\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    569\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_tolerant \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    571\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(include_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39min_keys\n\u001b[0;32m    572\u001b[0m     ):\n\u001b[1;32m--> 573\u001b[0m         tensordict \u001b[38;5;241m=\u001b[39m module(tensordict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_tolerant \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_td \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mtensordicts:\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:328\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\utils.py:372\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[1;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m _skip_existing\u001b[38;5;241m.\u001b[39mget_mode()\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 372\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(_self, tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    374\u001b[0m     _skip_existing\u001b[38;5;241m.\u001b[39mset_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:1218\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1216\u001b[0m in_keys \u001b[38;5;241m=\u001b[39m indent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1217\u001b[0m out_keys \u001b[38;5;241m=\u001b[39m indent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDictModule failed with operation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1220\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:1190\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   1185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome tensors that are necessary for the module call may \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot have not been found in the input tensordict: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe following inputs are None: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnone_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1188\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors_out, (\u001b[38;5;28mdict\u001b[39m, TensorDictBase)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m   1192\u001b[0m     key \u001b[38;5;129;01min\u001b[39;00m tensors_out \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_keys\n\u001b[0;32m   1193\u001b[0m ):\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors_out, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:1174\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[1;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m   1166\u001b[0m         tensordict\u001b[38;5;241m.\u001b[39m_get_tuple_maybe_non_tensor(\n\u001b[0;32m   1167\u001b[0m             _unravel_key_to_tuple(in_key),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m in_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys\n\u001b[0;32m   1172\u001b[0m     )\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1174\u001b[0m     tensors_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_module(tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tensors_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m         tensors_out \u001b[38;5;241m=\u001b[39m ()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\tensordict\\nn\\common.py:1133\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[1;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod_kwargs)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39mtensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod)(\u001b[38;5;241m*\u001b[39mtensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\modules\\models\\multiagent.py:135\u001b[0m, in \u001b[0;36mMultiAgentNetBase.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 135\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_forward_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# If parameters are not shared, each agent has its own network\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_params:\n",
      "File \u001b[1;32mc:\\Users\\FX643778\\AppData\\Local\\anaconda3\\envs\\pythorch_class\\lib\\site-packages\\torchrl\\modules\\models\\multiagent.py:446\u001b[0m, in \u001b[0;36mMultiAgentMLP._pre_forward_check\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pre_forward_check\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents:\n\u001b[1;32m--> 446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-agent network expected input with shape[-2]=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         )\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;66;03m# If the model is centralized, agents have full observability\u001b[39;00m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentralized:\n",
      "\u001b[1;31mValueError\u001b[0m: Multi-agent network expected input with shape[-2]=3, but got torch.Size([2, 21])"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# from tensordict import TensorDict\n",
    "# from pettingzoo.mpe import simple_reference_v3\n",
    "\n",
    "# env = simple_reference_v3.env(render_mode=\"human\", continuous_actions=True)\n",
    "# env.reset()\n",
    "\n",
    "# obs_vide = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# obs_agent1 = torch.tensor(obs_vide, dtype=torch.float32) \n",
    "# sum_reward = 0\n",
    "\n",
    "\n",
    "# # Pour avoir acces aux fonction render de petting zoo il faut etre en mode ACS donc on doit traiter les agent 1 par 1\n",
    "# # C'est ce que permet cette boucle on traite l'agent 0 puis 1 en boucle\n",
    "# for agent in env.agent_iter():\n",
    "#     env.render()\n",
    "#     observation, reward, termination, truncation, info = env.last()\n",
    "#     sum_reward += reward/2\n",
    "\n",
    "#     if termination or truncation:\n",
    "#         action = None\n",
    "#     else:\n",
    "#         if agent == \"agent_0\":\n",
    "#             obs_agent0 = torch.tensor(observation, dtype=torch.float32)\n",
    "#         else:\n",
    "#             obs_agent1 = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "#         obs_agents = torch.stack([obs_agent0, obs_agent1], dim=0)\n",
    "#         input_td = TensorDict(\n",
    "#             source={(\"agent\", \"observation\"): obs_agents},\n",
    "#             batch_size=[2], \n",
    "#         ) \n",
    "#         out = policy(input_td)\n",
    "\n",
    "#         if agent == \"agent_0\":\n",
    "#             action = out.get((\"agent\", \"action\"))[0]\n",
    "#         else:\n",
    "#             action = out.get((\"agent\", \"action\"))[1]\n",
    "#         action = action.detach().cpu().numpy()\n",
    "#     env.step(action)\n",
    "# env.close()\n",
    "\n",
    "# print(sum_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythorch_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
